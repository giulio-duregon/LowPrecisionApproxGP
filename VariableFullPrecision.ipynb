{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from GPYTorch Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "data = torch.Tensor(loadmat('./elevators.mat/elevators.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "X = X - X.min(0)[0]\n",
    "X = 2 * (X / X.max(0)[0]) - 1\n",
    "y = data[:, -1]\n",
    "\n",
    "\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.base_covar_module = ScaleKernel(RBFKernel())\n",
    "        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=train_x[:500, :], likelihood=likelihood)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    model = model.cuda()\n",
    "#    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood.noise_covar.raw_noise\n",
      "torch.Size([1])\n",
      "mean_module.raw_constant\n",
      "torch.Size([])\n",
      "base_covar_module.raw_outputscale\n",
      "torch.Size([])\n",
      "base_covar_module.base_kernel.raw_lengthscale\n",
      "torch.Size([1, 1])\n",
      "covar_module.inducing_points\n",
      "torch.Size([500, 18])\n"
     ]
    }
   ],
   "source": [
    "for param_name, param in model.named_parameters():\n",
    "    print(param_name)\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 0.799\n",
      "Iter 2/50 - Loss: 0.793\n",
      "Iter 3/50 - Loss: 0.787\n",
      "Iter 4/50 - Loss: 0.782\n",
      "Iter 5/50 - Loss: 0.777\n",
      "Iter 6/50 - Loss: 0.772\n",
      "Iter 7/50 - Loss: 0.767\n",
      "Iter 8/50 - Loss: 0.763\n",
      "Iter 9/50 - Loss: 0.758\n",
      "Iter 10/50 - Loss: 0.753\n",
      "Iter 11/50 - Loss: 0.749\n",
      "Iter 12/50 - Loss: 0.744\n",
      "Iter 13/50 - Loss: 0.740\n",
      "Iter 14/50 - Loss: 0.736\n",
      "Iter 15/50 - Loss: 0.731\n",
      "Iter 16/50 - Loss: 0.727\n",
      "Iter 17/50 - Loss: 0.722\n",
      "Iter 18/50 - Loss: 0.718\n",
      "Iter 19/50 - Loss: 0.714\n",
      "Iter 20/50 - Loss: 0.709\n",
      "Iter 21/50 - Loss: 0.705\n",
      "Iter 22/50 - Loss: 0.701\n",
      "Iter 23/50 - Loss: 0.697\n",
      "Iter 24/50 - Loss: 0.692\n",
      "Iter 25/50 - Loss: 0.688\n",
      "Iter 26/50 - Loss: 0.684\n",
      "Iter 27/50 - Loss: 0.680\n",
      "Iter 28/50 - Loss: 0.676\n",
      "Iter 29/50 - Loss: 0.671\n",
      "Iter 30/50 - Loss: 0.667\n",
      "Iter 31/50 - Loss: 0.663\n",
      "Iter 32/50 - Loss: 0.659\n",
      "Iter 33/50 - Loss: 0.654\n",
      "Iter 34/50 - Loss: 0.650\n",
      "Iter 35/50 - Loss: 0.646\n",
      "Iter 36/50 - Loss: 0.642\n",
      "Iter 37/50 - Loss: 0.638\n",
      "Iter 38/50 - Loss: 0.633\n",
      "Iter 39/50 - Loss: 0.629\n",
      "Iter 40/50 - Loss: 0.625\n",
      "Iter 41/50 - Loss: 0.621\n",
      "Iter 42/50 - Loss: 0.617\n",
      "Iter 43/50 - Loss: 0.612\n",
      "Iter 44/50 - Loss: 0.608\n",
      "Iter 45/50 - Loss: 0.604\n",
      "Iter 46/50 - Loss: 0.600\n",
      "Iter 47/50 - Loss: 0.596\n",
      "Iter 48/50 - Loss: 0.591\n",
      "Iter 49/50 - Loss: 0.587\n",
      "Iter 50/50 - Loss: 0.583\n",
      "Wall time: 51.6 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "def train():\n",
    "    for i in range(50):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, 50, loss.item()))\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# See dkl_mnist.ipynb for explanation of this flag\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "with gpytorch.settings.max_preconditioner_size(10), torch.no_grad():\n",
    "    preds = model(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE: 0.0731787383556366\n"
     ]
    }
   ],
   "source": [
    "print('Test MAE: {}'.format(torch.mean(torch.abs(preds.mean - test_y))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
