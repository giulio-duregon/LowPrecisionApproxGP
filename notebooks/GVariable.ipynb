{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# This will control precision of Symeig / Cholenski Lin Alg Operations\n",
    "gpytorch.settings.linalg_dtypes(default=torch.float16)\n",
    "#TODO: How to ensure that all operations, data, etc. follow this precision constraint?\n",
    "\n",
    "# Sample code for Batch Learning\n",
    "def get_DataLoaders(train_x, train_y, test_x, test_y):\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(test_x, test_y)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=500, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_and_test_approximate_gp(model_cls, train_x, train_y, test_y, train_loader, test_loader, num_epochs):\n",
    "    inducing_points = torch.randn(128, train_x.size(-1), dtype=train_x.dtype, device=train_x.device)\n",
    "    model = model_cls(inducing_points)\n",
    "    \n",
    "    # Likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    # Objective -> Variational Inference Uses ELBO\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.numel())\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()) + list(likelihood.parameters()), lr=0.1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        likelihood = likelihood.cuda()\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    # TQDM is just a progress bar for training\n",
    "    epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=f\"Training {model_cls.__name__}\")\n",
    "    \n",
    "    for i in epochs_iter:\n",
    "        # Within each iteration, we will go over each minibatch of data\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = -mll(output, y_batch)\n",
    "            epochs_iter.set_postfix(loss=loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    means = torch.tensor([0.])\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            preds = model(x_batch)\n",
    "            means = torch.cat([means, preds.mean.cpu()])\n",
    "    means = means[1:]\n",
    "    error = torch.mean(torch.abs(means - test_y.cpu()))\n",
    "    print(f\"Test {model_cls.__name__} MAE: {error.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extends ApproximateGP\n",
    "class StandardApproximateGP(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        # Needs a Variational Distribution + Variation Strategy\n",
    "        #TODO: Which ones do we need to implmenet?\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(-2))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "        )\n",
    "        super().__init__(variational_strategy) # Pass strategy upwards\n",
    "        \n",
    "        # Then Kernel and Mean as Normal\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = InducingPointKernel(gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()), inducing_points=torch.empty(1), likelihood=likelihood)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_select_points(model, inducing_point_candidates, train_x, train_y, mll):\n",
    "    \"\"\"\n",
    "    Inplace Addition of Inducing Point\n",
    "    \n",
    "    Output: inducing_point_candidates - Tensor | Returns remaining candidate set of inducing points\n",
    "    \"\"\"\n",
    "    # TODO: Fix Potential dimension errors\n",
    "    # look at .resize(), torch.cat, len(train)\n",
    "    # TODO: Pop train_x_copy index\n",
    "    # Get current MLL\n",
    "\n",
    "    random_indices = np.random.permutation(len(inducing_point_candidates))\n",
    "    inducing_points = model.covar_module.inducing_points\n",
    "    \n",
    "    # Get MLL from current inducing points\n",
    "    with torch.no_grad():\n",
    "            output = model(train_x)\n",
    "            current_model_mll = mll(output, train_y)\n",
    "\n",
    "    # While we haven't found a point\n",
    "    for index in random_indices:\n",
    "        rnd = inducing_point_candidates[index].reshape(1,-1) # TODO: Make this better\n",
    "        \n",
    "        # Grab a point at random, calculate its likelihood\n",
    "        temp = torch.cat((inducing_points, rnd),dim=0)\n",
    "        \n",
    "        # Update the inducing point kernel\n",
    "        model.covar_module.inducing_points = torch.nn.Parameter(temp,requires_grad=False)\n",
    "\n",
    "        # Get MLL for model with candidate inducing point\n",
    "        with torch.no_grad(): \n",
    "            rnd_point_mll = mll(model(train_x), train_y)\n",
    "\n",
    "        # If we've increased our likelihood, we've found our point\n",
    "        if rnd_point_mll.sum() > current_model_mll.sum():\n",
    "            # Catch edge case where we grab the last index\n",
    "            if index+1 == len(inducing_point_candidates):\n",
    "                return inducing_point_candidates[0:index]\n",
    "            else:\n",
    "                return torch.cat((inducing_point_candidates[0:index], inducing_point_candidates[index+1:]),dim=0) \n",
    "\n",
    "        \n",
    "    # If we couldn't increase our likelihood, get rid of the last appended inducing point\n",
    "    model.covar_module.inducing_points = torch.nn.Parameter(temp[:-1],requires_grad=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train_x = torch.linspace(0, 1, 100)\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * math.sqrt(0.04)\n",
    "train_x = train_x.reshape(-1,1)\n",
    "train_y = train_y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1]) torch.Size([100, 1]) torch.Size([100, 1])\n",
      "tensor(0.9496, grad_fn=<MeanBackward0>)\n",
      "Iter 1/50 - Loss: 0.950\n",
      "tensor(0.7553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.7072, grad_fn=<MeanBackward0>)\n",
      "tensor(0.6677, grad_fn=<MeanBackward0>)\n",
      "tensor(0.6290, grad_fn=<MeanBackward0>)\n",
      "tensor(0.5896, grad_fn=<MeanBackward0>)\n",
      "Iter 6/50 - Loss: 0.590\n",
      "tensor(0.5493, grad_fn=<MeanBackward0>)\n",
      "tensor(0.5084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.4667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.4244, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3814, grad_fn=<MeanBackward0>)\n",
      "Iter 11/50 - Loss: 0.381\n",
      "Failed to add inducing point, breaking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/gpytorch/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,model)\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "max_inducing_points = 1000\n",
    "inducing_point_candidates = train_x.detach().clone().reshape(-1,1)\n",
    "training_iter = 1000\n",
    "print(inducing_point_candidates.size(),train_x.size(), train_y.size())\n",
    "\n",
    "for i in range(max_inducing_points):\n",
    "    # If haven't gotten any inducing points, grab a random one\n",
    "    if len(inducing_point_candidates) == len(train_x):\n",
    "        random_index = np.random.randint(0, len(train_x))\n",
    "        first_inducing_point = train_x[random_index].detach().clone().reshape(1,-1) # Get\n",
    "        model.covar_module.inducing_points = torch.nn.Parameter(first_inducing_point, requires_grad=False) # Set\n",
    "        # Remove Selected Point from candidate set\n",
    "        inducing_point_candidates = torch.cat((inducing_point_candidates[:random_index], inducing_point_candidates[random_index + 1 :]),dim=0)\n",
    "        \n",
    "    elif len(model.covar_module.inducing_points) >= max_inducing_points:\n",
    "        print(f\"Reached limit of inducing points: we have {len(model.covar_module.inducing_points)} points with a maximum of {max_inducing_points}\")\n",
    "        break    \n",
    "    else:\n",
    "        inducing_point_candidates = greedy_select_points(model,inducing_point_candidates,train_x,train_y,mll)\n",
    "        if inducing_point_candidates is None:\n",
    "            # We've failed to find a point that increases our Likelihood\n",
    "            print(\"Failed to add inducing point, breaking\")\n",
    "            break\n",
    "        \n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    mll.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    print(loss.mean())\n",
    "    loss.mean().backward()\n",
    "    if i % 5 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, 50, loss.mean()))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    # TODO: Should we be training even if we stop grabbing inducing points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('likelihood.noise_covar.raw_noise', tensor([-1.1085])),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.lower_bound',\n",
       "              tensor(1.0000e-04)),\n",
       "             ('likelihood.noise_covar.raw_noise_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('mean_module.raw_constant', tensor(-0.0521)),\n",
       "             ('covar_module.inducing_points',\n",
       "              tensor([[0.9091],\n",
       "                      [0.5051],\n",
       "                      [0.6465],\n",
       "                      [0.1818],\n",
       "                      [0.2424],\n",
       "                      [0.5253],\n",
       "                      [0.8687],\n",
       "                      [0.9697],\n",
       "                      [0.0404],\n",
       "                      [0.4242],\n",
       "                      [0.7475]])),\n",
       "             ('covar_module.base_kernel.raw_outputscale', tensor(-0.5443)),\n",
       "             ('covar_module.base_kernel.base_kernel.raw_lengthscale',\n",
       "              tensor([[0.6043]])),\n",
       "             ('covar_module.base_kernel.base_kernel.raw_lengthscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.base_kernel.base_kernel.raw_lengthscale_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('covar_module.base_kernel.raw_outputscale_constraint.lower_bound',\n",
       "              tensor(0.)),\n",
       "             ('covar_module.base_kernel.raw_outputscale_constraint.upper_bound',\n",
       "              tensor(inf)),\n",
       "             ('covar_module.likelihood.noise_covar.raw_noise',\n",
       "              tensor([-1.1085])),\n",
       "             ('covar_module.likelihood.noise_covar.raw_noise_constraint.lower_bound',\n",
       "              tensor(1.0000e-04)),\n",
       "             ('covar_module.likelihood.noise_covar.raw_noise_constraint.upper_bound',\n",
       "              tensor(inf))])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood.noise_covar.raw_noise\n",
      "Parameter containing:\n",
      "tensor([-1.2103], requires_grad=True)\n",
      "mean_module.raw_constant\n",
      "Parameter containing:\n",
      "tensor(-0.0336, requires_grad=True)\n",
      "covar_module.inducing_points\n",
      "Parameter containing:\n",
      "tensor([0.8281, 0.2827, 0.5352, 0.5454, 0.2727, 0.8184, 0.6162, 0.8584, 0.6362,\n",
      "        0.9194, 0.8887, 0.5557, 0.4243, 0.6265, 0.3535, 0.3232, 0.9292, 0.1515,\n",
      "        0.3738, 0.6665, 0.0909, 0.9897, 0.3435, 0.1313, 0.1919, 0.0000, 0.7578,\n",
      "        0.2930, 0.1010, 0.8687, 0.7173, 0.4949, 0.0101, 0.7070, 0.4546, 0.2424,\n",
      "        0.8486, 0.8989, 0.4343, 0.3333, 0.0202, 0.4041, 0.4849],\n",
      "       dtype=torch.float16)\n",
      "covar_module.base_kernel.raw_outputscale\n",
      "Parameter containing:\n",
      "tensor(-0.6861, requires_grad=True)\n",
      "covar_module.base_kernel.base_kernel.raw_lengthscale\n",
      "Parameter containing:\n",
      "tensor([[0.4648]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param_name, param in model.named_parameters():\n",
    "    print(param_name)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,2,3,4],[5,6,7,8]])\n",
    "b = torch.Tensor([[10,11],[12,14]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]]) tensor([[[10., 11.],\n",
      "         [12., 14.]]])\n",
      "torch.Size([2, 2, 2]) torch.Size([1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = a.reshape(-1,2,2)\n",
    "b = b.reshape(-1,2,2)\n",
    "print(a,b)\n",
    "print(a.size(),b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.cat((a,b),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2.],\n",
       "         [3., 4.]],\n",
       "\n",
       "        [[5., 6.],\n",
       "         [7., 8.]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.Tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'LowPrecisionApproxGP'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/giulio/LowPrecisionApproxGP/notebooks/GVariable.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/giulio/LowPrecisionApproxGP/notebooks/GVariable.ipynb#ch0000018?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mLowPrecisionApproxGP\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m greedy_select_points, greedy_train\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'LowPrecisionApproxGP'"
     ]
    }
   ],
   "source": [
    "from LowPrecisionApproxGP.util import greedy_select_points, greedy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
