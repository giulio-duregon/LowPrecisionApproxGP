{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "from LowPrecisionApproxGP.model.inducing_point_kernel import VarPrecisionInducingPointKernel\n",
            "import gpytorch\n",
            "from gpytorch.kernels.inducing_point_kernel import InducingPointKernel\n",
            "import torch\n",
            "from LowPrecisionApproxGP.util.GreedyTrain import greedy_train\n",
            "from LowPrecisionApproxGP.model.models import VarPrecisionModel\n",
            "from LowPrecisionApproxGP import load_bikes, load_road3d"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "# Define Model\n",
            "class ExactGPModel(gpytorch.models.ExactGP):\n",
            "    def __init__(self, train_x, train_y, likelihood):\n",
            "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
            "        self.mean_module = gpytorch.means.ConstantMean()\n",
            "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
            "\n",
            "    def forward(self, x):\n",
            "        mean_x = self.mean_module(x)\n",
            "        covar_x = self.covar_module(x)\n",
            "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 15,
         "metadata": {
            "pycharm": {
               "name": "#%%\n"
            }
         },
         "outputs": [],
         "source": [
            "# dtype = torch.float32\n",
            "# train_x, train_y = train_x.to(dtype), train_y.to(dtype)\n",
            "# # Create Likelihood / Model\n",
            "# likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
            "# model = ExactGPModel(train_x, train_y, likelihood, dtype)\n",
            "# mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,model)\n",
            "\n",
            "# # Set to training mode\n",
            "# model.train()\n",
            "# likelihood.train()\n",
            "\n",
            "# # Use the adam optimizer\n",
            "# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
            "# # Train Model\n",
            "# model = greedy_train((train_x,train_y), model, mll,max_inducing_points,training_iter,dtype=dtype,Use_Max=True,J=20,max_Js=100)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 16,
         "metadata": {},
         "outputs": [],
         "source": [
            "train, test = load_bikes()\n",
            "train_x, train_y = train\n",
            "# # Create Likelihood / Model\n",
            "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
            "model = ExactGPModel(train_x, train_y, likelihood)\n",
            "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood,model)\n",
            "\n",
            "# Set to training mode\n",
            "model.train()\n",
            "likelihood.train()\n",
            "\n",
            "# Use the adam optimizer\n",
            "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
            "# Train Model\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 17,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  ", Message:Iteration:1/200, Average_Loss:49155.90303754907\n",
                  ", Message:Iteration:2/200, Average_Loss:45745.34508727801\n",
                  ", Message:Iteration:3/200, Average_Loss:42649.18711158417\n",
                  ", Message:Iteration:4/200, Average_Loss:39840.85640701716\n",
                  ", Message:Iteration:5/200, Average_Loss:37294.67875391016\n",
                  ", Message:Iteration:6/200, Average_Loss:34987.523529762875\n",
                  ", Message:Iteration:7/200, Average_Loss:32897.47672513231\n",
                  ", Message:Iteration:8/200, Average_Loss:31004.469894043214\n",
                  ", Message:Iteration:9/200, Average_Loss:29289.3252570493\n",
                  ", Message:Iteration:10/200, Average_Loss:27733.82984946021\n",
                  ", Message:Iteration:11/200, Average_Loss:26321.366742178892\n",
                  ", Message:Iteration:12/200, Average_Loss:25036.933543962277\n",
                  ", Message:Iteration:13/200, Average_Loss:23867.031461482784\n",
                  ", Message:Iteration:14/200, Average_Loss:22799.47694993811\n",
                  ", Message:Iteration:15/200, Average_Loss:21823.235354062195\n",
                  ", Message:Iteration:16/200, Average_Loss:20928.383370297754\n",
                  ", Message:Iteration:17/200, Average_Loss:20106.05522533984\n",
                  ", Message:Iteration:18/200, Average_Loss:19348.360658555786\n",
                  ", Message:Iteration:19/200, Average_Loss:18648.248078493143\n",
                  ", Message:Iteration:20/200, Average_Loss:17999.4193109507\n",
                  ", Message:Iteration:21/200, Average_Loss:17396.180590759024\n",
                  ", Message:Iteration:22/200, Average_Loss:16833.3593619451\n",
                  ", Message:Iteration:23/200, Average_Loss:16306.26974554876\n",
                  ", Message:Iteration:24/200, Average_Loss:15810.707763923101\n",
                  ", Message:Iteration:25/200, Average_Loss:15342.942624203399\n",
                  ", Message:Iteration:26/200, Average_Loss:14899.670183887149\n",
                  ", Message:Iteration:27/200, Average_Loss:14477.935601481646\n",
                  ", Message:Iteration:28/200, Average_Loss:14075.057587420699\n",
                  ", Message:Iteration:29/200, Average_Loss:13688.69219138932\n",
                  ", Message:Iteration:30/200, Average_Loss:13316.813339660474\n",
                  ", Message:Iteration:31/200, Average_Loss:12957.68128255693\n",
                  ", Message:Iteration:32/200, Average_Loss:12609.841369875423\n",
                  ", Message:Iteration:33/200, Average_Loss:12272.130749732216\n",
                  ", Message:Iteration:34/200, Average_Loss:11943.655889172678\n",
                  ", Message:Iteration:35/200, Average_Loss:11623.76108073275\n",
                  ", Message:Iteration:36/200, Average_Loss:11311.982772542313\n",
                  ", Message:Iteration:37/200, Average_Loss:11008.068811556546\n",
                  ", Message:Iteration:38/200, Average_Loss:10711.962908537776\n",
                  ", Message:Iteration:39/200, Average_Loss:10423.78780881911\n",
                  ", Message:Iteration:40/200, Average_Loss:10143.768841615998\n",
                  ", Message:Iteration:41/200, Average_Loss:9872.161412816862\n",
                  ", Message:Iteration:42/200, Average_Loss:9609.176596068679\n",
                  ", Message:Iteration:43/200, Average_Loss:9354.977802780411\n",
                  ", Message:Iteration:44/200, Average_Loss:9109.689498605949\n",
                  ", Message:Iteration:45/200, Average_Loss:8873.411870471096\n",
                  ", Message:Iteration:46/200, Average_Loss:8646.190857630103\n",
                  ", Message:Iteration:47/200, Average_Loss:8427.98917600753\n",
                  ", Message:Iteration:48/200, Average_Loss:8218.674335369658\n",
                  ", Message:Iteration:49/200, Average_Loss:8018.017891909799\n",
                  ", Message:Iteration:50/200, Average_Loss:7825.715463263433\n",
                  ", Message:Iteration:51/200, Average_Loss:7641.407795423439\n",
                  ", Message:Iteration:52/200, Average_Loss:7464.705522792135\n",
                  ", Message:Iteration:53/200, Average_Loss:7295.204225263488\n",
                  ", Message:Iteration:54/200, Average_Loss:7132.4999013558845\n",
                  ", Message:Iteration:55/200, Average_Loss:6976.198630826503\n",
                  ", Message:Iteration:56/200, Average_Loss:6825.928105507292\n",
                  ", Message:Iteration:57/200, Average_Loss:6681.346443243989\n",
                  ", Message:Iteration:58/200, Average_Loss:6542.146217998604\n",
                  ", Message:Iteration:59/200, Average_Loss:6408.0487719790935\n",
                  ", Message:Iteration:60/200, Average_Loss:6278.7922263973915\n",
                  ", Message:Iteration:61/200, Average_Loss:6154.127868094566\n",
                  ", Message:Iteration:62/200, Average_Loss:6033.808317407806\n",
                  ", Message:Iteration:63/200, Average_Loss:5917.589099943293\n",
                  ", Message:Iteration:64/200, Average_Loss:5805.225724788446\n",
                  ", Message:Iteration:65/200, Average_Loss:5696.476062999884\n",
                  ", Message:Iteration:66/200, Average_Loss:5591.106238420563\n",
                  ", Message:Iteration:67/200, Average_Loss:5488.892004417866\n",
                  ", Message:Iteration:68/200, Average_Loss:5389.62712954478\n",
                  ", Message:Iteration:69/200, Average_Loss:5293.123604017339\n",
                  ", Message:Iteration:70/200, Average_Loss:5199.214220788385\n",
                  ", Message:Iteration:71/200, Average_Loss:5107.7610016724875\n",
                  ", Message:Iteration:72/200, Average_Loss:5018.6377727449535\n",
                  ", Message:Iteration:73/200, Average_Loss:4931.736329119481\n",
                  ", Message:Iteration:74/200, Average_Loss:4846.966991813236\n",
                  ", Message:Iteration:75/200, Average_Loss:4764.2447085158665\n",
                  ", Message:Iteration:76/200, Average_Loss:4683.502802953628\n",
                  ", Message:Iteration:77/200, Average_Loss:4604.675835254708\n",
                  ", Message:Iteration:78/200, Average_Loss:4527.711351090181\n",
                  ", Message:Iteration:79/200, Average_Loss:4452.5572739501495\n",
                  ", Message:Iteration:80/200, Average_Loss:4379.1715847658415\n",
                  ", Message:Iteration:81/200, Average_Loss:4307.51101029313\n",
                  ", Message:Iteration:82/200, Average_Loss:4237.538986210291\n",
                  ", Message:Iteration:83/200, Average_Loss:4169.219170702603\n",
                  ", Message:Iteration:84/200, Average_Loss:4102.520435609841\n",
                  ", Message:Iteration:85/200, Average_Loss:4037.4108936360717\n",
                  ", Message:Iteration:86/200, Average_Loss:3973.8614982870736\n",
                  ", Message:Iteration:87/200, Average_Loss:3911.844588029884\n",
                  ", Message:Iteration:88/200, Average_Loss:3851.3313569377137\n",
                  ", Message:Iteration:89/200, Average_Loss:3792.296906774132\n",
                  ", Message:Iteration:90/200, Average_Loss:3734.712186866034\n",
                  ", Message:Iteration:91/200, Average_Loss:3678.5507285342915\n",
                  ", Message:Iteration:92/200, Average_Loss:3623.7846736245374\n",
                  ", Message:Iteration:93/200, Average_Loss:3570.3877735213623\n",
                  ", Message:Iteration:94/200, Average_Loss:3518.331410470863\n",
                  ", Message:Iteration:95/200, Average_Loss:3467.5851242019717\n",
                  ", Message:Iteration:96/200, Average_Loss:3418.1188690954914\n",
                  ", Message:Iteration:97/200, Average_Loss:3369.9118492276507\n",
                  ", Message:Iteration:98/200, Average_Loss:3322.925708127337\n",
                  ", Message:Iteration:99/200, Average_Loss:3277.140773814279\n",
                  ", Message:Iteration:100/200, Average_Loss:3232.5207519361325\n",
                  ", Message:Iteration:101/200, Average_Loss:3189.045118024039\n",
                  ", Message:Iteration:102/200, Average_Loss:3146.67947471142\n",
                  ", Message:Iteration:103/200, Average_Loss:3105.3970354446324\n",
                  ", Message:Iteration:104/200, Average_Loss:3065.169837239316\n",
                  ", Message:Iteration:105/200, Average_Loss:3025.9739653593124\n",
                  ", Message:Iteration:106/200, Average_Loss:2987.7679630483813\n",
                  ", Message:Iteration:107/200, Average_Loss:2950.5387276444567\n",
                  ", Message:Iteration:108/200, Average_Loss:2914.249724363874\n",
                  ", Message:Iteration:109/200, Average_Loss:2878.8708155349896\n",
                  ", Message:Iteration:110/200, Average_Loss:2844.381692031404\n",
                  ", Message:Iteration:111/200, Average_Loss:2810.743473791348\n",
                  ", Message:Iteration:112/200, Average_Loss:2777.9355640941712\n",
                  ", Message:Iteration:113/200, Average_Loss:2745.9288016446803\n",
                  ", Message:Iteration:114/200, Average_Loss:2714.69328921009\n",
                  ", Message:Iteration:115/200, Average_Loss:2684.206372246547\n",
                  ", Message:Iteration:116/200, Average_Loss:2654.4377425399493\n",
                  ", Message:Iteration:117/200, Average_Loss:2625.363621609489\n",
                  ", Message:Iteration:118/200, Average_Loss:2596.9579242872505\n",
                  ", Message:Iteration:119/200, Average_Loss:2569.19822615686\n",
                  ", Message:Iteration:120/200, Average_Loss:2542.0597895930036\n",
                  ", Message:Iteration:121/200, Average_Loss:2515.522226860721\n",
                  ", Message:Iteration:122/200, Average_Loss:2489.561836947141\n",
                  ", Message:Iteration:123/200, Average_Loss:2464.1589093656166\n",
                  ", Message:Iteration:124/200, Average_Loss:2439.2928951991503\n",
                  ", Message:Iteration:125/200, Average_Loss:2414.944287269894\n",
                  ", Message:Iteration:126/200, Average_Loss:2391.0945880979134\n",
                  ", Message:Iteration:127/200, Average_Loss:2367.7266150321993\n",
                  ", Message:Iteration:128/200, Average_Loss:2344.824800037415\n",
                  ", Message:Iteration:129/200, Average_Loss:2322.373137457921\n",
                  ", Message:Iteration:130/200, Average_Loss:2300.3547191163166\n",
                  ", Message:Iteration:131/200, Average_Loss:2278.7565677162147\n",
                  ", Message:Iteration:132/200, Average_Loss:2257.5641676151668\n",
                  ", Message:Iteration:133/200, Average_Loss:2236.7654018391463\n",
                  ", Message:Iteration:134/200, Average_Loss:2216.348377743901\n",
                  ", Message:Iteration:135/200, Average_Loss:2196.300397212224\n",
                  ", Message:Iteration:136/200, Average_Loss:2176.6107161646846\n",
                  ", Message:Iteration:137/200, Average_Loss:2157.2701705968875\n",
                  ", Message:Iteration:138/200, Average_Loss:2138.2669723798963\n",
                  ", Message:Iteration:139/200, Average_Loss:2119.593291760929\n",
                  ", Message:Iteration:140/200, Average_Loss:2101.240058639868\n",
                  ", Message:Iteration:141/200, Average_Loss:2083.1975599974803\n",
                  ", Message:Iteration:142/200, Average_Loss:2065.459285035451\n",
                  ", Message:Iteration:143/200, Average_Loss:2048.017296887018\n",
                  ", Message:Iteration:144/200, Average_Loss:2030.864140748968\n",
                  ", Message:Iteration:145/200, Average_Loss:2013.9937552368367\n",
                  ", Message:Iteration:146/200, Average_Loss:1997.397807761204\n",
                  ", Message:Iteration:147/200, Average_Loss:1981.0724510177945\n",
                  ", Message:Iteration:148/200, Average_Loss:1965.0098171433558\n",
                  ", Message:Iteration:149/200, Average_Loss:1949.2045144387523\n",
                  ", Message:Iteration:150/200, Average_Loss:1933.6521356604565\n",
                  ", Message:Iteration:151/200, Average_Loss:1918.3453474145836\n",
                  ", Message:Iteration:152/200, Average_Loss:1903.2799120935163\n",
                  ", Message:Iteration:153/200, Average_Loss:1888.4512753003553\n",
                  ", Message:Iteration:154/200, Average_Loss:1873.8545527638073\n",
                  ", Message:Iteration:155/200, Average_Loss:1859.484267383361\n",
                  ", Message:Iteration:156/200, Average_Loss:1845.3362611064326\n",
                  ", Message:Iteration:157/200, Average_Loss:1831.405840456427\n",
                  ", Message:Iteration:158/200, Average_Loss:1817.6882581501493\n",
                  ", Message:Iteration:159/200, Average_Loss:1804.1803622798104\n",
                  ", Message:Iteration:160/200, Average_Loss:1790.8776322748688\n",
                  ", Message:Iteration:161/200, Average_Loss:1777.7750347454667\n",
                  ", Message:Iteration:162/200, Average_Loss:1764.8701787274565\n",
                  ", Message:Iteration:163/200, Average_Loss:1752.1583227703393\n",
                  ", Message:Iteration:164/200, Average_Loss:1739.635000542912\n",
                  ", Message:Iteration:165/200, Average_Loss:1727.2976190769741\n",
                  ", Message:Iteration:166/200, Average_Loss:1715.1428945950406\n",
                  ", Message:Iteration:167/200, Average_Loss:1703.1655343633024\n",
                  ", Message:Iteration:168/200, Average_Loss:1691.3638331266188\n",
                  ", Message:Iteration:169/200, Average_Loss:1679.7335213212484\n",
                  ", Message:Iteration:170/200, Average_Loss:1668.2716215134888\n",
                  ", Message:Iteration:171/200, Average_Loss:1656.9751740293589\n",
                  ", Message:Iteration:172/200, Average_Loss:1645.839936520116\n",
                  ", Message:Iteration:173/200, Average_Loss:1634.8644039051028\n",
                  ", Message:Iteration:174/200, Average_Loss:1624.0450342631582\n",
                  ", Message:Iteration:175/200, Average_Loss:1613.3783645440494\n",
                  ", Message:Iteration:176/200, Average_Loss:1602.8620695016984\n",
                  ", Message:Iteration:177/200, Average_Loss:1592.4936130339838\n",
                  ", Message:Iteration:178/200, Average_Loss:1582.2698509742002\n",
                  ", Message:Iteration:179/200, Average_Loss:1572.1873441396667\n",
                  ", Message:Iteration:180/200, Average_Loss:1562.2436138041169\n",
                  ", Message:Iteration:181/200, Average_Loss:1552.4372059204784\n",
                  ", Message:Iteration:182/200, Average_Loss:1542.7652751876724\n",
                  ", Message:Iteration:183/200, Average_Loss:1533.2231997962606\n",
                  ", Message:Iteration:184/200, Average_Loss:1523.8110830945077\n",
                  ", Message:Iteration:185/200, Average_Loss:1514.525176314099\n",
                  ", Message:Iteration:186/200, Average_Loss:1505.3615448490614\n",
                  ", Message:Iteration:187/200, Average_Loss:1496.3204081700114\n",
                  ", Message:Iteration:188/200, Average_Loss:1487.3967859974873\n",
                  ", Message:Iteration:189/200, Average_Loss:1478.5892582874003\n",
                  ", Message:Iteration:190/200, Average_Loss:1469.8947959410034\n",
                  ", Message:Iteration:191/200, Average_Loss:1461.3114474899426\n",
                  ", Message:Iteration:192/200, Average_Loss:1452.837072996725\n",
                  ", Message:Iteration:193/200, Average_Loss:1444.467870538783\n",
                  ", Message:Iteration:194/200, Average_Loss:1436.2018279002\n",
                  ", Message:Iteration:195/200, Average_Loss:1428.0378495658842\n",
                  ", Message:Iteration:196/200, Average_Loss:1419.9727424589062\n",
                  ", Message:Iteration:197/200, Average_Loss:1412.0027892451355\n",
                  ", Message:Iteration:198/200, Average_Loss:1404.1273633662925\n",
                  ", Message:Iteration:199/200, Average_Loss:1396.344505528027\n",
                  ", Message:Iteration:200/200, Average_Loss:1388.651036682452\n"
               ]
            }
         ],
         "source": [
            "max_iter = 200\n",
            "for i in range(max_iter):\n",
            "    optimizer.zero_grad()\n",
            "    mll.zero_grad()\n",
            "    # Output from model\n",
            "    output = model(train_x)\n",
            "\n",
            "    # Calc average loss and backprop gradients\n",
            "    loss = -mll(output, train_y)\n",
            "    loss.mean().backward()\n",
            "\n",
            "    print(\n",
            "    f\"Message:Iteration:{i+1}/{max_iter}, Average_Loss:{loss.mean().item()}\"\n",
            "    )\n",
            "    torch.cuda.empty_cache()\n",
            "\n",
            "    optimizer.step()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "interpreter": {
         "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
      },
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.13"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
